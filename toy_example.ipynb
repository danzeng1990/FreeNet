{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b60120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from train_utils.utils import models_clean,get_cosine_schedule_with_warmup,train_one_epoch\n",
    "from models.hrnet import create_model, create_model_path\n",
    "from train_utils import transforms\n",
    "from dataset import MixKeypoint\n",
    "from train_utils.augmentation import RandWeakAugment\n",
    "from train_utils.ssl_utils import train_loop_ssl_v2, train_loop_ssl_self_v2, \\\n",
    "    train_loop_ssl_self_feedback_v2, train_loop_ssl_feedback, train_loop_ssl_split_feedback, train_loop_ssl_uda, \\\n",
    "    train_loop_ssl_self_uda, train_loop_ssl_uda_split_feedback\n",
    "from train_utils.validation_mix import eval_group_pck,evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a626cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad55701",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "        description=__doc__)\n",
    "parser.add_argument('--name', default='mix', type=str, help='experiment name')\n",
    "parser.add_argument('--info', default=\"toy\", type=str, help='experiment info')\n",
    "parser.add_argument('--device', default='cuda:0', help='device')\n",
    "parser.add_argument('--data-root', default='../dataset', type=str, help='data path')\n",
    "parser.add_argument('--pretrained-model-path', default='./pretrained_weights',type=str, help='pretrained weights path')\n",
    "parser.add_argument('--output-dir', default='./experiment',type=str, help='output dir depends on the time')\n",
    "\n",
    "# SL params\n",
    "parser.add_argument('--lr', default=5e-4, type=float,help='initial learning rate, 5e-4 is the default value for training')\n",
    "# 指定接着从哪个epoch数开始训练\n",
    "parser.add_argument('--start-epoch', default=0, type=int, help='start epoch')\n",
    "# 训练的总epoch数\n",
    "parser.add_argument('--epochs', default=10, type=int, metavar='N', help='number of total epochs to run')\n",
    "# 针对torch.optim.lr_scheduler.MultiStepLR的参数\n",
    "parser.add_argument('--eval-epoch', default=1, type=int, help='train x epoch, evaluate once')\n",
    "parser.add_argument('--lr-steps', default=[5, 8], nargs='+', type=int,\n",
    "                    help='decrease lr every step-size epochs')\n",
    "# 针对torch.optim.lr_scheduler.MultiStepLR的参数\n",
    "parser.add_argument('--lr-gamma', default=0.1, type=float, help='decrease lr by a factor of lr-gamma')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSL params\n",
    "parser.add_argument('--total-steps', default=100, type=int, help='number of total steps to run')\n",
    "parser.add_argument('--eval-step', default=10, type=int, help='number of eval steps to run')\n",
    "parser.add_argument('--start-step', default=0, type=int,\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--warmup-steps', default=30, type=int, help='warmup steps')\n",
    "parser.add_argument('--student-wait-steps', default=0, type=int, help='warmup steps')\n",
    "parser.add_argument('--uda-steps', default=50, type=int, help='warmup steps of lambda-u')\n",
    "\n",
    "parser.add_argument('--teacher_lr', default=1e-5, type=float,help='initial learning rate, 1e-5 is the default value for training')\n",
    "parser.add_argument('--student_lr', default=1e-3, type=float,help='initial learning rate, 1e-3 is the default value for training')\n",
    "\n",
    "# AdamW的weight_decay参数\n",
    "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,metavar='W',\n",
    "                    help='weight decay (default: 1e-4)',dest='weight_decay')\n",
    "parser.add_argument('--grad-clip', default=1e9, type=float, help='gradient norm clipping')\n",
    "#\n",
    "parser.add_argument('--workers', default=8, type=int, help='number of workers for DataLoader')\n",
    "parser.add_argument('--batch-size', default=8, type=int, help='batch size of label data')\n",
    "parser.add_argument('--mu', default=1, type=int, help='batch size factor of unlabel data ')\n",
    "parser.add_argument('--seed', default=2, type=int, help='seed for initializing training')\n",
    "# animal body关键点信息\n",
    "parser.add_argument('--keypoints-path', default=\"./info/keypoints_definition.json\", type=str,\n",
    "                    help='keypoints_format.json path')\n",
    "parser.add_argument('--fixed-size', default=[256, 256], nargs='+', type=int, help='input size')\n",
    "# keypoints点数\n",
    "parser.add_argument('--num-joints', default=26, type=int, help='num_joints')\n",
    "parser.add_argument(\"--all_results\", action=\"store_true\",  help=\"if true, evaluation will also save 26 kps prediction\")\n",
    "parser.add_argument(\"--save-all-weights\", action=\"store_true\",  help=\"whether save weights of all the epochs\")\n",
    "\n",
    "parser.add_argument(\"--sl\",default=False,action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl\",default=True,action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_self\", default=False,action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_uda\", default=False,action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_feedback\",default=False, action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_self_feedback\",default=False, action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_self_uda\",default=False, action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_split_feedback\", default=False,action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--ssl_uda_split_feedback\",default=False, action=\"store_true\",help=\"experiment setting\")\n",
    "parser.add_argument(\"--feedback_shared\", default=False,action=\"store_true\",help=\"feedback loss for mixing training\")\n",
    "parser.add_argument(\"--feedback_ap_10k_exclusive\",default=False, action=\"store_true\",help=\"feedback loss for mixing training\")\n",
    "parser.add_argument(\"--feedback_animal_pose_exclusive\", default=False,action=\"store_true\",help=\"feedback loss for mixing training\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--amp\",default=True,action=\"store_true\",  help=\"Use torch.cuda.amp for mixed precision training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now_time = now.strftime(\"%Y-%m_%d_%H-%M-%S\")\n",
    "args.output_dir = os.path.join(args.output_dir,now_time)\n",
    "output_dir = args.output_dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "info_output_dir = os.path.join(output_dir,'info')\n",
    "if not os.path.exists(info_output_dir):\n",
    "    os.mkdir(info_output_dir)\n",
    "results_output_dir = os.path.join(output_dir,'results')\n",
    "if not os.path.exists(results_output_dir):\n",
    "    os.mkdir(results_output_dir)\n",
    "save_weights_output_dir = os.path.join(output_dir,'save_weights')\n",
    "if not os.path.exists(save_weights_output_dir):\n",
    "    os.mkdir(save_weights_output_dir)\n",
    "\n",
    "if args.workers > 0:\n",
    "    args.workers = min([os.cpu_count(), args.workers])\n",
    "else:\n",
    "    args.workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.seed is not None:\n",
    "    set_seed(args)\n",
    "with open(args.keypoints_path, \"r\") as f:\n",
    "    animal_kps_info = json.load(f)\n",
    "fixed_size = args.fixed_size\n",
    "heatmap_hw = (args.fixed_size[0] // 4, args.fixed_size[1] // 4)\n",
    "kps_weights = np.array(animal_kps_info[\"kps_weights\"],dtype=np.float32).reshape((args.num_joints,))\n",
    "\n",
    "data_transform = {\n",
    "    \"train_sl\": transforms.Compose([\n",
    "        transforms.LabelFormatTrans(extend_flag=True),\n",
    "        transforms.HalfBody(0.3, animal_kps_info[\"upper_body_ids\"], animal_kps_info[\"lower_body_ids\"]),\n",
    "        transforms.AffineTransform(scale=(0.65, 1.35), rotation=(-45, 45), fixed_size=fixed_size),\n",
    "        transforms.RandomHorizontalFlip(0.5, animal_kps_info[\"flip_pairs\"]),\n",
    "        transforms.KeypointToHeatMap(heatmap_hw=heatmap_hw, gaussian_sigma=2, keypoints_weights=kps_weights),\n",
    "        # RandWeakAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.LabelFormatTrans(extend_flag=True),\n",
    "        transforms.TransformMPL(args, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.LabelFormatTrans(extend_flag=True),\n",
    "        transforms.AffineTransform(scale=None, rotation=None, fixed_size=fixed_size),\n",
    "        transforms.KeypointToHeatMap(heatmap_hw=heatmap_hw, gaussian_sigma=2, keypoints_weights=kps_weights),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "train_dataset_info = [{\"dataset\":\"ap_10k\",\"mode\":\"train\"},{\"dataset\":\"animal_pose\",\"mode\":\"train\"}]\n",
    "val_dataset_info = [{\"dataset\":\"ap_10k\",\"mode\":\"val\"},{\"dataset\":\"animal_pose\",\"mode\":\"val\"}]\n",
    "data_root = args.data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050262fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sl:\n",
    "    train_dataset = MixKeypoint(root=data_root, merge_info=train_dataset_info, transform=data_transform['train_sl'])\n",
    "    val_dataset = MixKeypoint(root=data_root, merge_info=val_dataset_info, transform=data_transform['val'])\n",
    "    train_dataset.sample_few(num_ratio=0.01)\n",
    "    train_dataset.get_kps_num(args)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    nw = args.workers  # number of workers\n",
    "    print('Using %g dataloader workers' % nw)\n",
    "\n",
    "    base_weight_path = args.pretrained_model_path\n",
    "    model = create_model(num_joints=args.num_joints, weight_path=base_weight_path, load_pretrain_weights=True)\n",
    "    model.to(args.device)\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              sampler=SubsetRandomSampler(range(len(train_dataset))),\n",
    "                              pin_memory=True,\n",
    "                              num_workers=nw,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=train_dataset.collate_fn)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(params,\n",
    "                                  lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay\n",
    "                                  )\n",
    "    scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "    # learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma)\n",
    "    val_path = os.path.join(args.output_dir, \"info/val_log.txt\")\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        model.train()\n",
    "        mean_loss, lr = train_one_epoch(args, model, optimizer, train_loader, device=args.device, epoch=epoch,\n",
    "                                        warmup=True, scaler=scaler)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % args.eval_epoch == 0:\n",
    "            model_name = f\"model-{epoch}\"\n",
    "            model.eval()\n",
    "            # save weights\n",
    "            save_files = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch}\n",
    "            if args.amp:\n",
    "                save_files[\"scaler\"] = scaler.state_dict()\n",
    "            torch.save(save_files, \"./{}/save_weights/model-{}.pth\".format(args.output_dir, epoch))\n",
    "            # evaluate on the test dataset\n",
    "            val_oks_value, val_pck_value, oks_list, pck_list = evaluate(args=args, model_name=model_name,\n",
    "                                                                        dataset=val_dataset)\n",
    "            oks_dict = {key['dataset'] + '_' + key['mode']: val for key, val in\n",
    "                        zip(val_dataset.dataset_infos, oks_list)}\n",
    "            pck_dict = {key['dataset'] + '_' + key['mode']: val for key, val in\n",
    "                        zip(val_dataset.dataset_infos, pck_list)}\n",
    "\n",
    "            # 计算在shared keypoints 和 exclusive keypoint上的PCK\n",
    "            # 这里是AP-10K 和 Animal Pose\n",
    "            # shared keypoints: [0,1,4,11,12,13,14,15,16,17,18,21,22,23,24,25]\n",
    "            # exclusive keypoints:[[8],[2,3,6,7]]\n",
    "            shared_kp_index = [0, 1, 4, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25]\n",
    "            exclusive_kp_index_a = [8]\n",
    "            exclusive_kp_index_b = [2, 3, 6, 7]\n",
    "            avg_shared_kps_pck, _ = eval_group_pck(args, model_name, val_dataset, shared_kp_index, [0, 1])\n",
    "            ap_10k_exclusive_kps_pck, _ = eval_group_pck(args, model_name, val_dataset, exclusive_kp_index_a, [0])\n",
    "            animal_pose_exclusive_kps_pck, _ = eval_group_pck(args, model_name, val_dataset, exclusive_kp_index_b,\n",
    "                                                              [1])\n",
    "            # write into txt\n",
    "            with open(val_path, \"a\") as f:\n",
    "                # 写入的数据包括coco指标还有loss和learning rate\n",
    "                result_info = [\n",
    "                    f\"val_mean_oks:{val_oks_value}\",\n",
    "                    f\"val_mean_pck:{val_pck_value}\",\n",
    "                    f\"loss:{mean_loss}:.6f\",\n",
    "                    f\"learning_rate:{lr:.6f}\",\n",
    "                    f\"oks_dict: {' '.join([f'{k}: {v}' for k, v in oks_dict.items()])}\",\n",
    "                    f\"pck_dict: {' '.join([f'{k}: {v}' for k, v in pck_dict.items()])}\",\n",
    "                    f\"PCK on shared kps:{avg_shared_kps_pck:.4f}\",\n",
    "                    f\"PCK on exclusive kps a:{ap_10k_exclusive_kps_pck:.4f}\",\n",
    "                    f\"PCK on exclusive kps b:{animal_pose_exclusive_kps_pck:.4f}\"\n",
    "                ]\n",
    "                txt = \"epoch:{} {}\".format(epoch, '  '.join(result_info))\n",
    "                f.write(txt + \"\\n\")\n",
    "else:\n",
    "    train_label_dataset = MixKeypoint(root=data_root, merge_info=train_dataset_info,transform=data_transform['train'])\n",
    "    train_unlabel_dataset = MixKeypoint(root=data_root, merge_info=train_dataset_info,transform=data_transform['train'])\n",
    "    val_dataset = MixKeypoint(root=data_root, merge_info=val_dataset_info, transform=data_transform['val'])\n",
    "\n",
    "    train_label_dataset.sample_few(num_ratio=0.01)\n",
    "    train_unlabel_dataset.eliminate_repeated_data(train_label_dataset.valid_lists)\n",
    "    train_label_dataset.get_kps_num(args)\n",
    "    train_unlabel_dataset.get_kps_num(args)\n",
    "    print(\"Data num of label dataset: \",len(train_label_dataset))\n",
    "    print(\"Data num of unlabel dataset: \",len(train_unlabel_dataset))\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    nw = args.workers  # number of workers\n",
    "    print('Using %g dataloader workers' % nw)\n",
    "    #\n",
    "    base_weight_path = args.pretrained_model_path\n",
    "    t_model = create_model_path(num_joints=args.num_joints, weight_path=base_weight_path,\n",
    "                                weight_name=\"mix_SL_0.1.pth\")\n",
    "    t_model.to(args.device)\n",
    "    t_model.train()\n",
    "\n",
    "    s_model = create_model(num_joints=args.num_joints, weight_path=base_weight_path, load_pretrain_weights=True)\n",
    "    s_model.to(args.device)\n",
    "    s_model.train()\n",
    "\n",
    "    train_label_loader = DataLoader(train_label_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    sampler=SubsetRandomSampler(range(len(train_label_dataset))),\n",
    "                                    pin_memory=True,\n",
    "                                    num_workers=nw,\n",
    "                                    drop_last=False,\n",
    "                                    collate_fn=train_label_dataset.collate_fn_mpl)\n",
    "    train_unlabel_loader = DataLoader(train_unlabel_dataset,\n",
    "                                      batch_size=batch_size * args.mu,\n",
    "                                      sampler=SubsetRandomSampler(range(len(train_unlabel_dataset))),\n",
    "                                      pin_memory=True,\n",
    "                                      num_workers=nw,\n",
    "                                      drop_last=False,\n",
    "                                      collate_fn=train_unlabel_dataset.collate_fn_mpl)\n",
    "\n",
    "    t_params = [p for p in t_model.parameters() if p.requires_grad]\n",
    "    s_params = [p for p in s_model.parameters() if p.requires_grad]\n",
    "\n",
    "    t_optimizer = torch.optim.AdamW(t_params, lr=args.teacher_lr, weight_decay=args.weight_decay)\n",
    "    s_optimizer = torch.optim.AdamW(s_params, lr=args.student_lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    t_scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "    s_scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "\n",
    "    # learning rate scheduler\n",
    "    t_scheduler = get_cosine_schedule_with_warmup(t_optimizer,\n",
    "                                                  args.warmup_steps,\n",
    "                                                  args.total_steps)\n",
    "    s_scheduler = get_cosine_schedule_with_warmup(s_optimizer,\n",
    "                                                  args.warmup_steps,\n",
    "                                                  args.total_steps,\n",
    "                                                  args.student_wait_steps)\n",
    "\n",
    "    # train\n",
    "    if args.ssl:\n",
    "        train_loop_ssl_v2(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                          s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_self:\n",
    "        train_loop_ssl_self_v2(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                               s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_uda:\n",
    "        train_loop_ssl_uda(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                           s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_feedback:\n",
    "        train_loop_ssl_feedback(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                                s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_self_feedback:\n",
    "        train_loop_ssl_self_feedback_v2(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                                        s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_self_uda:\n",
    "        train_loop_ssl_self_uda(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                                s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_split_feedback:\n",
    "        train_loop_ssl_split_feedback(args, train_label_loader, train_unlabel_loader, t_model, s_model, t_optimizer,\n",
    "                                      s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)\n",
    "    elif args.ssl_uda_split_feedback:\n",
    "        train_loop_ssl_uda_split_feedback(args, train_label_loader, train_unlabel_loader, t_model, s_model,t_optimizer,\n",
    "                                          s_optimizer,t_scheduler, s_scheduler, t_scaler, s_scaler, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.save_all_weights:\n",
    "    models_clean(root_path=args.output_dir,save_num=5,has_oks=True,has_pck=True)\n",
    "old_name = args.output_dir\n",
    "new_name = f\"./experiment/{args.name}_{args.info}_{os.path.basename(old_name)}\"\n",
    "os.rename(old_name,new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b1c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
